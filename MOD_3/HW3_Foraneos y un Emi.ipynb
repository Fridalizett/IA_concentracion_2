{"cells":[{"cell_type":"markdown","metadata":{"id":"wpSG0SQ5swk1"},"source":["# Equipo: Foraneos y un Emi\n","\n","## Carolina Arratia Camacho - A01367552\n","\n","## Emiliano Mendoza Nieto - A01706083\n","\n","## Frida Lizett Zavala Pérez - A01275226\n","\n","## Fabián González Vera - A01367585\n","\n","## Jazzareth Bernal Martínez- A01367882"],"id":"wpSG0SQ5swk1"},{"cell_type":"markdown","metadata":{"id":"037e89c8"},"source":["## TC3007B\n","### Text Generation\n","\n","<br>\n","\n","### Simple LSTM Text Generator using WikiText-2\n","\n","<br>\n","\n","- Objective:\n","    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n","    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n","    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n","    \n","<br>\n","\n","- Instructions:\n","    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n","\n","    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n","\n","    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n","\n","    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n","\n","    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n","\n","    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n","    \n","<br>\n","\n","- Evaluation Criteria:\n","    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n","\n","    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n","\n","    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n","\n","    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n","\n"],"id":"037e89c8"},{"cell_type":"markdown","metadata":{"id":"G05jp2Vux5Pw"},"source":["Instalación de paquetes y librerías para el procesamiento de lenguaje natural."],"id":"G05jp2Vux5Pw"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15477,"status":"ok","timestamp":1700870637760,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"3eb4b117","outputId":"bc08200f-11f9-4152-e356-e9071e6f62ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting portalocker\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Installing collected packages: portalocker\n","Successfully installed portalocker-2.8.2\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"]}],"source":["!pip install portalocker\n","!pip install --upgrade portalocker\n","\n","import numpy as np\n","#PyTorch libraries\n","import torch\n","import torchtext\n","from torchtext.datasets import WikiText2\n","# Dataloader library\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.utils.data.dataset import random_split\n","# Libraries to prepare the data\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.functional import to_map_style_dataset\n","# neural layers\n","from torch import nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","import random"],"id":"3eb4b117"},{"cell_type":"markdown","metadata":{"id":"HeCZ9J2d3rzn"},"source":["Asigna a device la cadena 'cuda' si hay una GPU disponible, y 'cpu' si no hay GPU. Esta línea permite seleccionar automáticamente el dispositivo de procesamiento disponible en el sistema."],"id":"HeCZ9J2d3rzn"},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700870637761,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"6d8ff971"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"id":"6d8ff971"},{"cell_type":"markdown","metadata":{"id":"0IYNPFpO4Ksf"},"source":["Se carga automáticamente los conjuntos de datos de entrenamiento, validación y prueba del conjunto de datos WikiText-2."],"id":"0IYNPFpO4Ksf"},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700870637761,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"f3288ce5"},"outputs":[],"source":["train_dataset, val_dataset, test_dataset = WikiText2()"],"id":"f3288ce5"},{"cell_type":"markdown","metadata":{"id":"76Spm7JL4dQQ"},"source":["Primero se define un tokenizador el cual divide el texto en tokens básicos del inglés, y una función generadora que con el tokenizador creado anteriormente produce los tokens a partir de la data, a medida que se van necesitando para evitar cargarlos de una vez todos en la memoria.\n"],"id":"76Spm7JL4dQQ"},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700870637762,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"fc4c7dbd"},"outputs":[],"source":["tokeniser = get_tokenizer('basic_english')\n","def yield_tokens(data):\n","    for text in data:\n","        yield tokeniser(text)"],"id":"fc4c7dbd"},{"cell_type":"markdown","metadata":{"id":"rnaUl6Ns6FGK"},"source":["Se construye un vocabuliario a partir de lso tokens generados en el conjunto de datos de entrenamiento. Se considera la generación de tokens especiales. ```(\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\")```.\n","\n","También se establece un índice predenterminado del vocabulario guíadose del token ```\"<unk>\"```. Para usarse en caso de encontrar algún token que no se encuentre en el vocabulario."],"id":"rnaUl6Ns6FGK"},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4698,"status":"ok","timestamp":1700870642453,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"2c2cb068"},"outputs":[],"source":["# Build the vocabulary\n","vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n","#set unknown token at position 0\n","vocab.set_default_index(vocab[\"<unk>\"])"],"id":"2c2cb068"},{"cell_type":"markdown","metadata":{"id":"58C-1JBX6Qf8"},"source":["Se genera la función ```data_process``` la cual prepara los datos para usarlo en el modelo posteriormente, en este caso el modelo de generación de texto."],"id":"58C-1JBX6Qf8"},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7151,"status":"ok","timestamp":1700870649597,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"134b832b"},"outputs":[],"source":["seq_length = 50 #longitud de las secuencias\n","\n","# Función para procesar datos\n","def data_process(raw_text_iter, seq_length = 50):\n","    # Tokeniza y convierte en tensores cada elemento del iterable de texto\n","    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n","\n","     # Combina los tensores en uno solo, eliminando tensores vacíos\n","    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n","#     target_data = torch.cat(d)\n","\n","    # Crea conjuntos basados en el tensor combinado\n","    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n","            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n","\n","# Crea tensores para cada set de datos (validación, train y test)\n","x_train, y_train = data_process(train_dataset, seq_length)\n","x_val, y_val = data_process(val_dataset, seq_length)\n","x_test, y_test = data_process(test_dataset, seq_length)"],"id":"134b832b"},{"cell_type":"markdown","metadata":{"id":"7t_-INuq6O2A"},"source":["Crear conjuntos de datos (`TensorDataset`) para entrenamiento, validación y prueba utilizando los tensores previamente procesados (`x_train`, `y_train`, `x_val`, `y_val`, `x_test`, `y_test`). Estos conjuntos están listos para ser utilizados con PyTorch para el entrenamiento y evaluación de modelos de lenguaje."],"id":"7t_-INuq6O2A"},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1700870649598,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"4b54c04d"},"outputs":[],"source":["train_dataset = TensorDataset(x_train, y_train)\n","val_dataset = TensorDataset(x_val, y_val)\n","test_dataset = TensorDataset(x_test, y_test)"],"id":"4b54c04d"},{"cell_type":"markdown","metadata":{"id":"c-ySIC7Q6RlV"},"source":["Estos loaders se utilizan en el entrenamiento de modelos para proporcionar lotes de datos al modelo en cada iteración. La opción `shuffle=True` garantiza que los lotes se seleccionen de manera aleatoria en cada época, lo que es útil para mejorar la generalización del modelo."],"id":"c-ySIC7Q6RlV"},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1700870649598,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"f4d400fb"},"outputs":[],"source":["batch_size = 64  # choose a batch size that fits your computation resources\n","                # definir el lote de procesamiento\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) #carga datos al conjunto de entrenamiento y los combina,\n","                                                                                              #elimina el lote final si en menor que el batch_size\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True) #carga datos al conjunto de validación con las mismas\n","                                                                                          #configuraciones que el el conjunto de entrenamiento.\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)#carga datos al conjunto de prueba con las mismas\n","                                                                                          #configuraciones que el el conjunto de entrenamiento."],"id":"f4d400fb"},{"cell_type":"markdown","metadata":{"id":"NSEutO1O6SlJ"},"source":["Se define la clase para el modelo LSTM, el cual se utiliza para procesar las secuencias de texto y poder predecir la palabra siguiente."],"id":"NSEutO1O6SlJ"},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":208,"status":"ok","timestamp":1700870649774,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"59c63b01"},"outputs":[],"source":["# Define the LSTM model\n","# Feel free to experiment\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(LSTMModel, self).__init__()\n","\n","        # capa de embedding para convertir índices de palabras en vectores densos\n","        self.embeddings = nn.Embedding(vocab_size, embed_size)\n","\n","        # tamaño oculto de la capa LSTM y número de capas\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # capa LSTM\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        # capa de salida, transforma la salida al tamaño del vocabulario\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, text, hidden):\n","        embeddings = self.embeddings(text)\n","        output, hidden = self.lstm(embeddings, hidden)\n","        decoded = self.fc(output)\n","        #retorna la salida y el estado oculto\n","        return decoded, hidden\n","\n","    def init_hidden(self, batch_size):\n","        # inicializa el estado oculto de la capa LSTM\n","        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n","                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n","\n","\n","#tamaño del vocabulario\n","vocab_size = len(vocab)\n","#tamaño del embeding\n","emb_size = 100\n","neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n","num_layers = 2 # the number of nn.LSTM layers\n","#creear una instancia del modelo lstm\n","model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"],"id":"59c63b01"},{"cell_type":"markdown","metadata":{"id":"EweiZhZ8uWRk"},"source":["Definición de la función de entrenamineto. Esta función se ejecuta durante múltiples épocas y realiza la retropopágación y la actualización de parámetros para ajustar el modelo a los datos de entrenamiento."],"id":"EweiZhZ8uWRk"},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700870649774,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"},"user_tz":360},"id":"215eabb9"},"outputs":[],"source":["def train(model, epochs, optimiser, loss_function, train_loader):\n","    # mueve el procesamineto del modelo a la GPU, si este está disponible.\n","    model = model.to(device=device)\n","\n","    # establece el modelo en modo de entrenamiento\n","    model.train()\n","\n","    # ciclo de entrenamiento por las épocas\n","    for epoch in range(epochs):\n","        total_loss = 0\n","\n","        # inicializa el estado oculto antes de cada época\n","        hidden = model.init_hidden(batch_size)\n","\n","        # itera sobre los lotes de datos de entrenamiento\n","        for i, (data, targets) in enumerate(train_loader):\n","            # se mueven los datos y etiquetas a la GPU\n","            data, targets = data.to(device), targets.to(device)\n","\n","            # reinicia los gradientes\n","            optimiser.zero_grad()\n","\n","            # Inicializa los estados ocultos para la primera entrada del lote\n","            hidden = tuple([each.data for each in hidden])\n","\n","            # Propagación hacia adelante\n","            output, hidden = model(data, hidden)\n","\n","            # Calcula la pérdida\n","            loss = loss_function(output.view(-1, vocab_size), targets.view(-1))\n","            total_loss += loss.item()\n","\n","            # Backpropagation\n","            loss.backward()\n","\n","            # Actualiza los parámetros\n","            optimiser.step()\n","\n","        # Imprime información útil\n","        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n","\n"],"id":"215eabb9"},{"cell_type":"markdown","metadata":{"id":"wARI2k6i6iOW"},"source":["En la siguiente sección se entrena el modelo (se manda a llamar la función de entrenamiento), usando como optizador el  algoritmo Adam y cross entropy como función de pérdida.\n","\n"],"id":"wARI2k6i6iOW"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aa9c84ce","executionInfo":{"status":"ok","timestamp":1700872026177,"user_tz":360,"elapsed":1376408,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"}},"outputId":"3115afb6-e711-4bfa-9c4d-0ad9992d19c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 7.071770977973938\n","Epoch 2/50, Loss: 6.5374506033957\n","Epoch 3/50, Loss: 6.287883224338293\n","Epoch 4/50, Loss: 6.104196494072676\n","Epoch 5/50, Loss: 5.972130925953389\n","Epoch 6/50, Loss: 5.871773736923933\n","Epoch 7/50, Loss: 5.789107673615217\n","Epoch 8/50, Loss: 5.717801616340876\n","Epoch 9/50, Loss: 5.655099970847369\n","Epoch 10/50, Loss: 5.598686393350363\n","Epoch 11/50, Loss: 5.547684736549854\n","Epoch 12/50, Loss: 5.500482453405857\n","Epoch 13/50, Loss: 5.456367491185665\n","Epoch 14/50, Loss: 5.415236841887236\n","Epoch 15/50, Loss: 5.377145034819842\n","Epoch 16/50, Loss: 5.342250720411539\n","Epoch 17/50, Loss: 5.309158261120319\n","Epoch 18/50, Loss: 5.277817095816135\n","Epoch 19/50, Loss: 5.247809153050184\n","Epoch 20/50, Loss: 5.219631105661392\n","Epoch 21/50, Loss: 5.192895857989788\n","Epoch 22/50, Loss: 5.166794969886541\n","Epoch 23/50, Loss: 5.14173918813467\n","Epoch 24/50, Loss: 5.1176565803587435\n","Epoch 25/50, Loss: 5.094218428432941\n","Epoch 26/50, Loss: 5.071738664060831\n","Epoch 27/50, Loss: 5.04964544698596\n","Epoch 28/50, Loss: 5.028739626705646\n","Epoch 29/50, Loss: 5.007912178337574\n","Epoch 30/50, Loss: 4.987279926985503\n","Epoch 31/50, Loss: 4.967294309288263\n","Epoch 32/50, Loss: 4.948275438696146\n","Epoch 33/50, Loss: 4.929456936568021\n","Epoch 34/50, Loss: 4.910885924100876\n","Epoch 35/50, Loss: 4.8930861979722975\n","Epoch 36/50, Loss: 4.875118110328913\n","Epoch 37/50, Loss: 4.857809935510159\n","Epoch 38/50, Loss: 4.840931205451488\n","Epoch 39/50, Loss: 4.824427767843008\n","Epoch 40/50, Loss: 4.80902107283473\n","Epoch 41/50, Loss: 4.793408120423555\n","Epoch 42/50, Loss: 4.7777132473886015\n","Epoch 43/50, Loss: 4.762252889573574\n","Epoch 44/50, Loss: 4.747806859761477\n","Epoch 45/50, Loss: 4.7328559823334215\n","Epoch 46/50, Loss: 4.719015458226204\n","Epoch 47/50, Loss: 4.705550852417946\n","Epoch 48/50, Loss: 4.691019193083048\n","Epoch 49/50, Loss: 4.677565791457892\n","Epoch 50/50, Loss: 4.6646159492433075\n"]}],"source":["# Call the train function\n","loss_function = nn.CrossEntropyLoss()# definición de la función de perdida\n","lr = 0.0005 # taza de aprendizaje\n","epochs = 50 # número de épocas\n","optimiser = optim.Adam(model.parameters(), lr=lr) # optimizador para ajustar los parámetros\n","train(model, epochs, optimiser, loss_function, train_loader)\n"],"id":"aa9c84ce"},{"cell_type":"markdown","metadata":{"id":"aQCfigXU6kW0"},"source":["Función para generar el texto, la cual recibe como párametrlos el modelo, el texto inicial, el número de palabras a generar y la temperatura, el cuál controla la aleatoriedad del proceso. Valores mayores a 1 hacen que sea más aleatoria, mientras que valores más próximos a 0 hacen que sea menos aleatorio.\n","\n","Esta funcion itera sobre el número de palabras, con cada iteración se agrega una palabra a la secuencia, esta secuencia es usada como el input para la siguiente iteración."],"id":"aQCfigXU6kW0"},{"cell_type":"code","execution_count":13,"metadata":{"id":"c8667411","executionInfo":{"status":"ok","timestamp":1700872026178,"user_tz":360,"elapsed":24,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"}}},"outputs":[],"source":["def generate_text(model, start_text, num_words, temperature):\n","    model.eval() # Indica al modelo que se va a realizar una evaluacion\n","    words = tokeniser(start_text) # Genera tokens a partir del texto inicial\n","    hidden = model.init_hidden(1) # Inicializa el estado oculto de las capas\n","\n","    for i in range(num_words):\n","        # Convierte la secuencia de palabras en tensores\n","        x = torch.tensor([[vocab[word] for word in words[-seq_length:]]], dtype=torch.long, device=device)\n","        y_pred, hidden = model(x, hidden) # Realiza forward propagation\n","        last_word_logits = y_pred[0][-1] # Obtiene el ultimo output\n","        p = F.softmax(last_word_logits / temperature, dim=0).detach().cpu().numpy() # Calcula la probabilidad\n","        word_index = np.random.choice(len(last_word_logits), p=p) # Selecciona las palabras\n","        words.append(vocab.lookup_token(word_index)) # Agrega palabra a la secuencia de palabras\n","\n","    return ' '.join(words)"],"id":"c8667411"},{"cell_type":"markdown","metadata":{"id":"kRtXqQ8nrOkN"},"source":["Ejemplos de textos generados cambiando el parámetro *temperature*."],"id":"kRtXqQ8nrOkN"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2884543","executionInfo":{"status":"ok","timestamp":1700872026611,"user_tz":360,"elapsed":442,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"}},"outputId":"4b4bd2cc-4000-40cc-d4b9-f2ed4954147a"},"outputs":[{"output_type":"stream","name":"stdout","text":["i like changing calculations as a sign of a <unk> and you ultimately . but what leigh broke the school was prompted by german clark @-@ e , a boss comics video song , legends and chains . hurlford reach overseas with sea mark , as a partial churrigueresque engine . the brazen ibis cannot be considered one of the episodes contest from the end of the incident , he then enjoyed to don ' s reign , in 1946 . i represented a concert @-@ <unk> gown and conductor before reinforcements . owing to the creation of actors and he sent\n","nice to meet you never <unk> her 1769 that bird will rooted 22 copies during defense time . he was appointed and causes to the charity . fowler then won the album , he would have hot a dvd in the michael edition . the fifty @-@ time @-@ minute sister campaigns cross inquiry against belgium and his past father , although he regarded this influences there . contemporary international chapters wrote , with the medway megaliths bc , one picked reveal in this game judge and paul hao taught the position as a mixed @-@ note victim . that day , she went\n","you should bring you <unk> . while chapter at the peak the old butterflies arrives in his later use on the girls john see himself traveller in hieroglyphic success his boxing series was present in the weekly meeting chapter , but directed manic @-@ sided radio relationship . while nbc ' s father , kamalakannan and john william waymon of wilfrid was the rejection of success with songs using the song and the relationship between ulysses was not uncommon . = = = digital moon = = = the two chaplains previously responded with senior populations . however , that the nucleus\n"]}],"source":["print(generate_text(model, start_text=\"I like\", num_words=100, temperature=1.0))\n","print(generate_text(model, start_text=\"Nice to meet you\", num_words=100, temperature=1.0))\n","print(generate_text(model, start_text=\"You should\", num_words=100, temperature=1.0))"],"id":"d2884543"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78eabe9e","executionInfo":{"status":"ok","timestamp":1700872026795,"user_tz":360,"elapsed":190,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"}},"outputId":"2ad1b1d3-e462-4603-de97-46bc3cb4d41d"},"outputs":[{"output_type":"stream","name":"stdout","text":["i don ' t like a <unk> . i . calospora , and i <unk> . he was <unk> in the second round of the same year , with the <unk> of the song . the album was released in the united states , and the album is a <unk> in the <unk> <unk> & <unk> . the final , which is the first time in the urn , the song was in the episode ' s music of the year and the second of the episode ' s final season . the album was released in the united states , and the third @-@\n"]}],"source":["print(generate_text(model, start_text=\"I don't like\", num_words=100, temperature=0.5))"],"id":"78eabe9e"},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cb126a2","executionInfo":{"status":"ok","timestamp":1700872027003,"user_tz":360,"elapsed":214,"user":{"displayName":"Frida Lizett Zavala Pérez","userId":"09520279524064139688"}},"outputId":"b610dddc-47b8-460c-aeaf-a7f3840f41d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["hello everyone , the <unk> , and <unk> <unk> . = = = <unk> = = = the <unk> of the <unk> is a <unk> of the <unk> , a <unk> <unk> , and <unk> <unk> . the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> ( <unk> <unk> ) , <unk> <unk> <unk> <unk> ( <unk> <unk> ) , <unk> <unk> ( <unk> <unk> ) , <unk> ( <unk> <unk> ) , <unk> ( <unk> <unk> ) , <unk> ( <unk> <unk> ) , <unk> ( <unk> <unk> )\n"]}],"source":["print(generate_text(model, start_text=\"Hello everyone\", num_words=100, temperature=0.2))\n"],"id":"1cb126a2"},{"cell_type":"markdown","source":["Al generar las predicciones nos percatamos que el parámetro temperature afecta en gran medida la forma en la que estos se generan, observando un mejor comportamiento cuándo este es 1, debido a que le da más soltura al modelo de elegir entre las palabras seleccionadas, y no es tan determinista."],"metadata":{"id":"SEMJntJuW4oW"},"id":"SEMJntJuW4oW"},{"cell_type":"markdown","source":["Este modelo se compone de 2 capas LSTM, 128 neuronas y un tamaño de embedding de 100. El learning rate es de 0.0005, usa el optimizador Adam, la función de pérdida es crossentropy y se entrenó con 50 épocas."],"metadata":{"id":"-9grJOEdgzxO"},"id":"-9grJOEdgzxO"},{"cell_type":"markdown","source":["La principal diferencia entre este modelo y el usado para la tarea de clasificación es el tipo de red, este modelo usa una red LSTM y el modelo de clasifición usa una RNN. A pesar de que ambas redes tienen la capacidad de \"recordar\" los valores que recibieron anteriormente, las redes RNN tienen problemas con secuencias largas.\n","\n","Las LSTM son un tipo de RNN, sin embargo puede retener mejor la información a largo plazo. Tienen una estructura más compleja que las RNN convencionales, la cual consiste en diversas entradas y salidas, una de las principales diferencias y al mismo tiempo ventajas de las LSTM, es que son capaces de retener o descartar selectivamente la información del estado oculto, esto es beneficioso para el problema de generación de texto ya que se puede olvidar de manera selectiva cierta información.\n"],"metadata":{"id":"IPRBASlJbm9j"},"id":"IPRBASlJbm9j"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1tAvbRIiyT5kmDUj-qUEwneSnUjW52WdG","timestamp":1700583093327}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":5}