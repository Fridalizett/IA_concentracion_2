{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Frida Lizett Zavala Pérez\n",
        "\n",
        "### A01275226\n",
        "\n",
        "# Clustering de Datos de Taxis con PySpark\n",
        "\n",
        "En este script, se utiliza Apache Spark para realizar clustering en datos de taxis de Nueva York. El proceso incluye la carga de datos desde Google Drive, la limpieza de los datos, la selección de características relevantes, la creación de un modelo de clustering K-means, y la evaluación de la calidad del clustering.\n",
        "\n",
        "El dataset seleccionado para la implementación fue extraido de kaggle, el cual es un registro de viajes de taxis de Nueva York en el año 2016. El archivo csv tiene un tamaño de 1.78 GB y cuenta con 19 columnas, de las cuales para prpósitos de esta implementacón se seleccionaron las más relevantes.\n",
        "\n"
      ],
      "metadata": {
        "id": "j_qyuFhdKd3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza la instalación de pyspark sobre el entorno virtual."
      ],
      "metadata": {
        "id": "61HHOSRGN8v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOL5B5LhiqzN",
        "outputId": "92145802-a9ac-4e14-d404-3c947ae262f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=1bfe8065903d23311fa6e401c49c10b43ef9765b46d7cf6a7839a533235e605f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montamos drive para poder acceder a la carpeta donde se encuentra alojado el conjunto de datos que usaremos. Enseguida se configuran las rutas a las carpetas lara poder usarlas en el código."
      ],
      "metadata": {
        "id": "q31-_Q3wOFmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "drive_path = '/content/gdrive/MyDrive/IA concentracion/Bloque 2/Mod 1/Entregable'\n",
        "data_path = os.path.join(drive_path, \"data\")\n",
        "df4_path = os.path.join(data_path, \"yellow_tripdata_2016-03.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqwJU8XYiy7Y",
        "outputId": "4fe4eb55-6666-4b27-fd33-5aa568131ae3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se instalan las librerías de pyspark que necesitaremos para el problema de clustering, en este caso contruiremos el modelo de agrupamiento con KMeans, del mismo modo añadimos las herramientas para su evaluación posterior.\n",
        "Se inicia la sesión de spark."
      ],
      "metadata": {
        "id": "7h_C8Oh8OX-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"TaxiClustering\").getOrCreate()"
      ],
      "metadata": {
        "id": "FzfirJixUGcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como en cualquier modelo, es necesario realizar una limpieza y análisis de datos previa para preparar el conjunto de datos y mejorar el desempeño del modelo generado, evitando sesgos importamtes. En este caso se realizó la eliminación de filas con valores nulos usando el método ```na.drop()```, esto ayuda a garantizar que el dataset esté completo y no existan valores faltantes que afecten el entrenamiento del modelo.\n",
        "También se aplicó un filtrado de valores atípicos para eliminar valores que pudiesen generar ruido en columnas específicas. Tales como valores inválidos o no probables.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iSN-XTOpO01F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = spark.read.csv(df4_path, header=True, inferSchema=True)\n",
        "\n",
        "df = df4\n",
        "# Eliminar filas con valores nulos\n",
        "df = df.na.drop()\n",
        "\n",
        "# Filtrar valores atípicos en columnas específicas\n",
        "    (col(\"passenger_count\") > 0) &\n",
        "    (col(\"trip_distance\") > 0) &\n",
        "    (col(\"pickup_longitude\").isNotNull()) &\n",
        "    (col(\"pickup_latitude\").isNotNull()) &\n",
        "    (col(\"dropoff_longitude\").isNotNull()) &\n",
        "    (col(\"payment_type\").isNotNull()) &\n",
        "    (col(\"fare_amount\").isNotNull()) &\n",
        "    (col(\"tip_amount\").isNotNull()) &\n",
        "    (col(\"total_amount\").isNotNull())\n",
        ")"
      ],
      "metadata": {
        "id": "x0FFPFvaUJ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como paso siguiente se seleccionan las columnas que aporten información más relevante, las cuales representas características como el número de pasajeros, la distancia del viaje, las coordenadas de ascenso y descenso, tipo de pago, y las cantidades de la tarifa y propinas.\n"
      ],
      "metadata": {
        "id": "oHTsClRYQSPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar las columnas relevantes para el clustering\n",
        "selected_columns = [\n",
        "    \"passenger_count\",\n",
        "    \"trip_distance\",\n",
        "    \"pickup_longitude\",\n",
        "    \"pickup_latitude\",\n",
        "    \"dropoff_longitude\",\n",
        "    \"payment_type\",\n",
        "    \"fare_amount\",\n",
        "    \"tip_amount\",\n",
        "    \"total_amount\"\n",
        "]"
      ],
      "metadata": {
        "id": "L_48iXx3UaQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haciendo uso de ``` VectorAssembler``` se combinan las columnas anteriores en una misma llamada \"features\", que representa un vector de características."
      ],
      "metadata": {
        "id": "eqzTFM65R2D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar las características en una sola columna de características vectoriales\n",
        "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
        "df = assembler.transform(df)"
      ],
      "metadata": {
        "id": "u6F3MDx3Ubgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para generar el aprendizaje y la validación del modelo, se dividen los datos en el conjunto de entrenamiento y prueba, con una proporción de 80% y 20% respectivamente."
      ],
      "metadata": {
        "id": "4jhbsZuvR2Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el conjunto de datos en entrenamiento y validación\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=1)"
      ],
      "metadata": {
        "id": "dfbQGbqZUkXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguimos con el entrenamiento del modelo, para el cual se crea una instancia KMeans con un número predeterminado de clústers, en este caso fueron 3. y se entrena con el conjunto de entrenamiento.\n"
      ],
      "metadata": {
        "id": "HaDuJYbVR2Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo K-means\n",
        "kmeans = KMeans(k=3, seed=1)  # Puedes ajustar el número de clústeres (k) según sea necesario\n",
        "model = kmeans.fit(train_df)"
      ],
      "metadata": {
        "id": "vic6FfxrUp9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se usan las instancias del modelo previamente entrenado para generar predicciones con el conjunto de prueba."
      ],
      "metadata": {
        "id": "4drjL_93SbHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M1ZX7Vq0ZA0T"
      },
      "outputs": [],
      "source": [
        "# Hacer predicciones en el conjunto de prueba\n",
        "predictions = model.transform(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente de hace la evaluación del modelo a partir de las predicciones hechas.\n",
        "Una métrica empleada comunmente para la evaluación de modelos de agrupamiento es *Silhouette with squared euclidean distance* dónde se mide de -1 a 1,muentras más grande sea, mejor se encuentran definidos cada uno de los clústers. En es te caso los resultados obtenidos son buenos ya que se obtuvo un valor de 0.99.\n",
        "\n",
        "Se muestran los resultados imprimiendo los centroides, así cómo un resumen del modelo en general, (número de clústers y número de puntos).\n"
      ],
      "metadata": {
        "id": "7TlNbsG7Spep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar la calidad del clustering utilizando el evaluador de clustering\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Centroides:\")\n",
        "centers = model.clusterCenters()\n",
        "for center in centers:\n",
        "    print(center)\n",
        "\n",
        "# Imprimir algunas métricas adicionales\n",
        "print(\"Métricas adicionales:\")\n",
        "print(\"Número de clústeres:\", len(centers))\n",
        "print(\"Tamaño de los clústeres:\")\n",
        "cluster_sizes = predictions.groupBy(\"prediction\").count().collect()\n",
        "for cluster_size in cluster_sizes:\n",
        "    print(f\"Clúster {cluster_size['prediction']}: {cluster_size['count']} puntos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTWT1eBN6Oto",
        "outputId": "2d3a7818-deed-4a35-d2ae-786f40a91497"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.9999995561860565\n",
            "Centroides:\n",
            "[  1.66119218   3.48950621 -72.93964662  40.18120795 -73.05734287\n",
            "   1.33819889  12.69799042   1.7888611   15.9448992 ]\n",
            "[ 1.00000000e+00  1.90726288e+07 -7.39957962e+01  4.07612038e+01\n",
            " -7.39957886e+01  2.00000000e+00  2.50000000e+00  0.00000000e+00\n",
            "  4.30000000e+00]\n",
            "[ 2.00000000e+00  8.33008320e+06 -7.40049286e+01  4.07300186e+01\n",
            " -7.39946136e+01  2.00000000e+00  9.00000000e+00  0.00000000e+00\n",
            "  9.80000000e+00]\n",
            "Métricas adicionales:\n",
            "Número de clústeres: 3\n",
            "Tamaño de los clústeres:\n",
            "Clúster 2: 1 puntos\n",
            "Clúster 0: 2429123 puntos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente se la por terminada la sesión de spark."
      ],
      "metadata": {
        "id": "9mkzukFfT58R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Terminar la sesión de Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "4FYQCH6J6iWk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se adjunta un dashboard dónde se visualiza información relevante del dataset. De dichos gráficos se puede ontener un análisis y entendimiento de los datos más exahustivo ya que nos permite observar mejor diversos comportaminetos o patrones que podrían estar presentes de entrada en el conjunto de datos.\n",
        "\n"
      ],
      "metadata": {
        "id": "_TocMxtCU0Sx"
      }
    }
  ]
}